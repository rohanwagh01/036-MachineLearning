{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 Fall 2021 Homework 9 Colab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanwagh01/036-MachineLearning/blob/main/MIT_6_036_Fall_2021_Homework_9_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbQZlAqGY60Y"
      },
      "source": [
        "#Homework 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5TajqU2VtDp"
      },
      "source": [
        "!rm -rf code_for_hw12* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw12/code_for_hw12.zip --no-check-certificate\n",
        "!unzip -q code_for_hw12.zip\n",
        "!mv code_for_hw12/* .\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.linear_model import LogisticRegression, ElasticNet, LinearRegression\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import neighbors, datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8HU1fizgs1Q"
      },
      "source": [
        "# 2) Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNVei2l0g2mJ"
      },
      "source": [
        "###Decisions, decisions, decisions...\n",
        "In this section, we will be looking at a dataset that aims to determine whether a wine is of good quality or bad quality based off of some features. We'll train a decision tree for our classification problem here. We first load in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1diz3RU3HI"
      },
      "source": [
        "all_data = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
        "all_data.loc[all_data[\"quality\"] <= 5, \"quality\"] = 0\n",
        "all_data.loc[all_data[\"quality\"] > 5, \"quality\"] = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz6WDBexAAAz",
        "outputId": "b90c5e73-54e6-471d-e8ad-b19305a47be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "features = all_data.columns[:-1]\n",
        "print(features)\n",
        "wine_data = all_data[features]\n",
        "wine_labels = all_data[[\"quality\"]]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
            "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
            "       'pH', 'sulphates', 'alcohol'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFlauVzNBsWr"
      },
      "source": [
        "train_wine_data, test_wine_data, train_wine_labels, test_wine_labels =\\\n",
        "   train_test_split(wine_data, wine_labels, stratify = wine_labels, test_size = 0.3, random_state = 20) #do not touch random state"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTzIXxqdm9ZC"
      },
      "source": [
        "###Hyperparameter Search\n",
        "Tuning parameters is very important when training decision trees because we run the risk of overfitting. For this reason, we'll have to use cross validation to ensure that the parameters we're setting in our decision tree not only works for our training data but validation data as well. We'll use sci-kit learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) package as our model and use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to run a parameter search to find our optimal parameters.\n",
        "\n",
        "We can first define what parameters we want to search over, and what the values we should try are. GridSearchCV will try all possible combinations of these parameter settings and save which parameters do the best. Specifically, we will try different values for `criterion`, `min_samples_split`, `min_samples_leaf`, and `max_depth`. Please read the documentation in DecisionTreeClassifier to understand what these parameters are.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypuqlelrCA3D"
      },
      "source": [
        "#decision tree params\n",
        "dt_params = {\n",
        "    \"criterion\":[\"gini\", \"entropy\"],\n",
        "    \"min_samples_split\": [2, 5, 10, 50, 100, 150, 200, 300], \n",
        "    \"min_samples_leaf\": [2, 5, 10, 20, 100, 150, 200, 250],\n",
        "    \"max_depth\": [3, 5, 10, 50, 100]\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKcANPFoi2xN"
      },
      "source": [
        "####Conceptual Questions\n",
        "1) How does increasing min_samples_split affect training accuracy? Test accuracy?\n",
        "\n",
        "2) How does increasing min_samples_leaf affect training accuracy? Test error?\n",
        "\n",
        "3) How does increasing max_depth affect training accuracy? Test error?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emHBp-o5AotZ"
      },
      "source": [
        "Look at the GridSearchCV documentation and DecisionTreeClassifier documentation to run a hyperparameter search on the wine dataset. Search over the space defined in `dt_params`. For the next section, due to the stochastic nature of creating training and test datasets, multiple solutions are accepted. If you believe that your method is correct but the solution isn't being accepted, please post a private message directed to staff on Piazza with your methodology and results. Due to the stochastic nature of creating training and test datasets, multiple solutions are accepted, but you should fix the parameter **random_state=10** for the DecisionTreeClassifier to ensure you get the staff output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nirpQv27CDuN"
      },
      "source": [
        "#todo, use GridSearchCV to find the best parameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dFqaS2uCvpK"
      },
      "source": [
        "best_params = ['l1', 10, 'liblinear']\n",
        "print(best_params)\n",
        " \n",
        "#Now we will use all of the training data to relearn a decision classifier\n",
        "decision_clf = DecisionTreeClassifier(**best_params)\n",
        "decision_clf.fit(train_wine_data, train_wine_labels)\n",
        "print(decision_clf.score(test_wine_data, test_wine_labels)) #find the score of the test data, metric is mean accuracy from docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZH9VSYZv6Nj"
      },
      "source": [
        "Now, we will use sci-kit learn's [plot tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) to visualize our learned decision tree.\n",
        "\n",
        "As an example, if our node is \n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "this means we are splitting along the \"free sulfur dioxide\" feature, there are 84 wines in this node that are classified as bad wine and 55 wines in this node that are classified as good wine. Thus, we output the label bad wine for this node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nypCd4TpDHrT"
      },
      "source": [
        "# #visualize the tree\n",
        "fig, ax = plt.subplots(figsize=(30, 30)) \n",
        "plot = plot_tree(decision_clf, feature_names = features, class_names=[\"bad wine\", \"good wine\"], fontsize=10, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7ZpEKT9Q-kT"
      },
      "source": [
        "Answer the questions on catsoop about the interpretation of this tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q81tLDjSFdU1"
      },
      "source": [
        "###Logistic Regression Hyperparameter Search\n",
        "Now, we want to compare our decision tree to a logistic regression to see if the decision tree is picking up some nonlinearities in the dataset. We do the same thing for a logistic regression: we first run a hyperparameter search for the best parameters for [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and compare the accuracies and interpretations. Please read the documentation for LogisticRegression to understand the settings in `lr_params`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIzqtpo3DRpf"
      },
      "source": [
        "lr_params = {\n",
        "        \"penalty\":[\"l1\", \"l2\"],\n",
        "        \"C\":[1e-3, 1e-2, 1e-1, 1, 5, 10, 100, 250],\n",
        "        \"solver\": [\"liblinear\"]\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOBk3bj3GBfr"
      },
      "source": [
        "####Conceptual Questions\n",
        "How does increasing $C$ affect training accuracy? Testing accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNvgqKk2VJ1Y"
      },
      "source": [
        "Now, run a hyperparameter search using GridSearchCV again, but for a logistic regression. Due to the stochastic nature of creating training and test datasets, multiple solutions are accepted, but you should fix the parameter **random_state=10** for the LogisticRegression to ensure you get the staff output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaFy5TPUDRlT"
      },
      "source": [
        "#reshape train_wine_labels to be 1D\n",
        "train_wine_labels_1d = np.array(train_wine_labels).reshape(len(train_wine_labels),)\n",
        "# TODO: Use GridSearchCV to find the best parameter values for a logistic regression classifier\n",
        "#hint, you might want to use train_wine_labels_1d as the input to this GridSearchCV, why?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md1vpFJlGV2b"
      },
      "source": [
        "lr_best_params = ['l1', 10, 'liblinear']\n",
        "print(lr_best_params) \n",
        "lr_classifier = LogisticRegression(**lr_best_params)\n",
        "lr_classifier.fit(train_wine_data, train_wine_labels_1d)\n",
        "print(lr_classifier.score(test_wine_data, test_wine_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h3eOuE5ZnNC"
      },
      "source": [
        "Similarly to when analyzing decision trees, we would like to determine which features play the most important role in classifying whether a wine is good or bad. Take a look at the coefficients of this model and determine which feature is the most important to classifying a good wine vs a bad wine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec8ajtMMu4lz"
      },
      "source": [
        "lr_coefficients = #todo: get the coefficients of the logistic regression\n",
        "# get the weights of the features of the wine data\n",
        "sorted_coeff_indices = np.argsort(np.abs(lr_coefficients))\n",
        "for i in sorted_coeff_indices[0]:\n",
        "    print(\"%s: %s\" %(features[i], lr_coefficients[0][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3N97zsXJbr"
      },
      "source": [
        "However, the coefficients as is don't provide much intuition on how important the features are, since the features could be of different scale. So if we have features that naturally take on very large values, they may artificially get a small coefficient. For that reason, we scale the data inputs and take a look at the coeffiecients of the scaled features.\n",
        "\n",
        "Now, what is the most important feature?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zj_znVnV86o"
      },
      "source": [
        "lr_classifier_scaled = LogisticRegression(**lr_best_params)\n",
        "lr_classifier_scaled.fit(scale(train_wine_data), train_wine_labels_1d)\n",
        "print(lr_classifier_scaled.score(scale(test_wine_data), test_wine_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGDdf6f6XNsl"
      },
      "source": [
        "lr_coefficients_scaled = #todo: get the coefficients of the scaled logistic regression model\n",
        "# get the weights of the scaled features of the wine data\n",
        "sorted_coeff_indices = np.argsort(np.abs(lr_coefficients_scaled))\n",
        "for i in sorted_coeff_indices[0]:\n",
        "    print(\"%s: %s\" %(features[i], lr_coefficients_scaled[0][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtVI0YdOb1cz"
      },
      "source": [
        "While we're still able to determine what features are important in classifying whether a wine is good or bad, we have no sense of what values these features need to take to classify whether a wine is good or bad, like we were able to do in a decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etuULtOsXQhn"
      },
      "source": [
        "###Random Forest Hyperparameter Search\n",
        "Finally, we do the same hyperparameter search for a random forest. Use GridSearchCV again to find the best setting of parameters from `rf_params`. Performing this search may take some time (around 5-10 minutes), so be patient! When initializing `RandomForestClassifier`, please use a **random state of 10**, to ensure that the output stays consistent with the solutions on Catsoop! \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuG4pR-drcJW"
      },
      "source": [
        "rf_params = {\n",
        "    \"n_estimators\":[50, 200],\n",
        "    \"criterion\":[\"gini\", \"entropy\"],\n",
        "    \"max_depth\":[2, 10, 50],\n",
        "    \"min_samples_leaf\":[2,10,50]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHFlsF19rwPy"
      },
      "source": [
        "rf_best_params = #TODO\n",
        "print(rf_best_params)\n",
        "rf_clf = RandomForestClassifier(**rf_best_params, random_state = 10) #do not touch random state\n",
        "rf_clf.fit(train_wine_data, train_wine_labels_1d)\n",
        "print(rf_clf.score(test_wine_data, test_wine_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNF6SY9tXqzP"
      },
      "source": [
        "We see that a big advantage of using sci-kit learn is its ability to quickly switch out different models we would like to use. All we need to change is the hyperparameters we want to search over and the model input to GridSearchCV!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASqJL_DYHKy"
      },
      "source": [
        "# 4) Nearest Neighbor Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cztWXq_SXcY"
      },
      "source": [
        "Implement the following distance functions below according to their specifications. They should compute the pairwise distance between a single sample of dimension (d,1)(d,1) and nn individual samples, each with dimension (d,1)(d,1), in an array of dimension (d,n)(d,n). This will make it easy to compute the distances between a single test sample and the entire training set in KNN. Numpy is preloaded for you, but you can't use np.linalg.norm.\n",
        "\n",
        "A) Implement the Euclidean distance below. Remember that the Euclidean distance between two dd-dimensional vectors is defined as :\n",
        "\n",
        "$$E(p,q) = \\sqrt{\\sum_{i=1}^{d}(p_i - q_i)^2}$$\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br6NiEHTuwMp"
      },
      "source": [
        "def euclidean(p,q):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        p is d by n array\n",
        "        q is d by 1 array\n",
        "    Returns :\n",
        "        (n,) (i.e. one-dimensional, n elements) array: the pairwise Euclidean distance of q with respect to individual samples in  p\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum(np.square(p-q),axis=0))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gBDw0Ano5hj",
        "outputId": "7ecf23a5-e4ce-4a14-daf2-b3f81904a01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_euclidean(): \n",
        "    passed = 0 \n",
        "    p = np.array([[10], [-4]]) \n",
        "    q = np.array([[0], [1]]) \n",
        "    c = np.array([11.180339887498949])\n",
        "    if np.isclose(euclidean(p,q), c): \n",
        "        passed += 1 \n",
        "    else: \n",
        "        print(\"failed test case 1\")\n",
        "    p = np.array([[0, 1, 1, -4, -2], [0, 1, 2, 9, -2]]) \n",
        "    q = np.array([[-2], [1]]) \n",
        "    c = np.array([2.23606797749979, 3.0, 3.1622776601683795, 8.246211251235321, 3.0])\n",
        "    if np.isclose(euclidean(p,q), c).all(): \n",
        "        passed += 1 \n",
        "    else: \n",
        "        print(\"failed test case 2\")\n",
        "    print(f\"passed {passed}/2 test cases\")\n",
        "test_euclidean()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed 2/2 test cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWnHzDRu1-j"
      },
      "source": [
        "B)Implement the Manhattan distance below. Remember that the Manhattan distance between two d-dimensional vectors is defined as :\n",
        "\n",
        "$$M(p,q) = \\sum_{i=1}^{d}|p_i - q_i|$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5i4d5j5ux6z"
      },
      "source": [
        "def manhattan(p,q):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        p is d by n array \n",
        "        q is d by 1 array\n",
        "    Returns :\n",
        "        (n,) (i.e. one-dimensional, n elements) array: the pairwise Manhattan distance of q with respect to individual samples in p\n",
        "    \"\"\"\n",
        "    return np.sum(np.absolute(p-q),axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4DPy3VYtgJC",
        "outputId": "46949032-5727-4a89-921b-2aa42db6ee08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_manhattan(): \n",
        "    passed = 0 \n",
        "    p = np.array([[10], [-4]]) \n",
        "    q = np.array([[0], [1]])\n",
        "    c = np.array([15]) \n",
        "    if np.isclose(manhattan(p,q), c): \n",
        "        passed += 1 \n",
        "    else: \n",
        "        print(\"failed test case 1\")\n",
        "    p = np.array([[0, 1, 1, -4, -2], [0, 1, 2, 9, -2]]) \n",
        "    q = np.array([[-2], [1]])\n",
        "    c = np.array([3, 3, 4, 10, 3])\n",
        "    if np.isclose(manhattan(p, q), c).all(): \n",
        "        passed += 1\n",
        "    else: \n",
        "        print(\"failed test case 2\") \n",
        "    print(f\"passed {passed}/2 test cases\")\n",
        "test_manhattan()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed 2/2 test cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkOi9fbvOWT"
      },
      "source": [
        "C) Implement the KNN class for classification below according to its specifications. Some functions that may come in handy are `np.argsort` and `np.bincount`. If you need a refresher on how KNNs work for classification, take a look at [the notes](https://lms.mitx.mit.edu/assets/courseware/v1/f0cc0c187c20a52b6ca4d967efdca4f0/asset-v1:MITx+6.036+2020_Fall+type@asset+block/notes_chapter_Non-parametric_methods.pdf). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxVyKBT4vN9-"
      },
      "source": [
        "class KNN:\n",
        "    def __init__(self, K, distance_metric, trainX, trainY):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            K is an int representing the number of closest neighbors to consider\n",
        "            distance_metric is one of euclidean or manhattan\n",
        "            trainX is d by n array\n",
        "            trainY is 1 by n array\n",
        "        \"\"\"\n",
        "        self.trainX = trainX\n",
        "        self.trainY = trainY\n",
        "        self.K = K\n",
        "        self.metric = distance_metric\n",
        "        \n",
        "    def calc_distances(self, testX):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            testX is d by m np array\n",
        "        Returns:\n",
        "            an m x n np array D where D[i, j] is the distance between test sample i and train sample j\n",
        "        \"\"\"\n",
        "        #look at each of m\n",
        "        return np.vstack([self.metric(self.trainX, testX[:, i:i+1]) for i in range(testX.shape[1])])\n",
        "\n",
        "    def find_top_neighbor_labels(self, dists):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            dists is  m x n np array D where D[i, j] is the distance between test sample i and train sample j\n",
        "        Returns:\n",
        "            an m x K np array L where L[i, j] is the label of the jth closest neighbor to test sample i\n",
        "            in case of ties, the neighbor which appears first in the training set is chosen\n",
        "        \"\"\"\n",
        "        ns = np.argsort(dists)[:, :self.K]\n",
        "        ns_l = self.trainY.squeeze()[ns]\n",
        "        return ns_l\n",
        "     \n",
        "    def predict(self, testX):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            testX is d by m np array\n",
        "        Returns:\n",
        "            predicted is (m,) np array P where P[i] is the predicted label for test sample i \n",
        "        \"\"\"\n",
        "        ns_l = self.find_top_neighbor_labels(self.calc_distances(testX))\n",
        "        pred = np.hstack([np.bincount(vals).argmax() for vals in ns_l])\n",
        "        return pred\n",
        "\n",
        "    def score(self, testX, testY):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            testX is d by m np array of input data\n",
        "            testY is 1 by m np array of labels for the input data\n",
        "        Returns:\n",
        "            a scalar: the accuracy of the KNN predictions across the test set\n",
        "        \"\"\"\n",
        "        return np.sum(self.predict(testX) == testY) / testY.shape[1]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBDNw5SUuSkV",
        "outputId": "2c08d732-f5aa-483a-f62e-ed267a0827bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import math\n",
        "\n",
        "trainx = np.array([[ 2., 16., 4., 3., 3., 6., 4.], [15., 15., 13., 13., 13., 12., 15.], [15., 3., 8., 13., 16., 16., 16.]])\n",
        "trainy = np.array([[0,1,2,2,1,0,1]])\n",
        "testx = np.array([[ 9., 15.,  0., 14., 16.], [ 9., 16.,  6., 13., 14.],[16., 15., 16., 16., 11.]])\n",
        "testy_1 = np.array([[0, 1, 2, 1, 0]]) \n",
        "knn1 = KNN(1, euclidean, trainx, trainy) \n",
        "knn1_1 = KNN(1, euclidean, testx, testy_1) \n",
        "knn3 = KNN(3, euclidean, trainx, trainy)\n",
        "knn5 = KNN(5, euclidean, trainx, trainy) \n",
        "\n",
        "passed = 0 \n",
        "print(\"testing knn.calc_distances\")\n",
        "knn1 = KNN(1, euclidean, trainx, trainy) \n",
        "dists = knn1.calc_distances(testx)\n",
        "expected = np.array([[9.273618495495704, 15.937377450509228, 10.246950765959598, 7.810249675906654, 7.211102550927978, 4.242640687119285, 7.810249675906654], [13.038404810405298, 12.083045973594572, 13.379088160259652, 12.529964086141668, 12.409673645990857, 9.899494936611665, 11.090536506409418], \n",
        "                     [9.273618495495704, 22.494443758403985, 11.357816691600547, 8.18535277187245, 7.615773105863909, 8.48528137423857, 9.848857801796104], [12.206555615733702, 13.30413469565007, 12.806248474865697, 11.40175425099138, 11.0, 8.06225774829855, 10.198039027185569], \n",
        "                     [14.594519519326424, 8.06225774829855, 12.409673645990857, 13.19090595827292, 13.96424004376894, 11.357816691600547, 13.038404810405298]])\n",
        "if np.isclose(dists, expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 1\")\n",
        "expected = np.array([[9.273618495495704, 13.038404810405298, 9.273618495495704, 12.206555615733702, 14.594519519326424], [15.937377450509228, 12.083045973594572, 22.494443758403985, 13.30413469565007, 8.06225774829855], \n",
        "                     [10.246950765959598, 13.379088160259652, 11.357816691600547, 12.806248474865697, 12.409673645990857], [7.810249675906654, 12.529964086141668, 8.18535277187245, 11.40175425099138, 13.19090595827292], \n",
        "                     [7.211102550927978, 12.409673645990857, 7.615773105863909, 11.0, 13.96424004376894], [4.242640687119285, 9.899494936611665, 8.48528137423857, 8.06225774829855, 11.357816691600547], \n",
        "                     [7.810249675906654, 11.090536506409418, 9.848857801796104, 10.198039027185569, 13.038404810405298]])\n",
        "dists = knn1_1.calc_distances(trainx)\n",
        "if np.isclose(dists, expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 2\")\n",
        "print(\"testing knn.find_top_neighbor_labels\")\n",
        "expected = np.array([[0], [0], [1], [0], [1]])\n",
        "dists = np.vstack([knn1.metric(knn1.trainX, testx[:, i:i+1]) for i in range(testx.shape[1])])\n",
        "actual = knn1.find_top_neighbor_labels(dists)\n",
        "if np.isclose(actual, expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 3\")\n",
        "expected = np.array([[0], [0], [0], [0], [0], [0], [0]])\n",
        "dists = np.vstack([knn1_1.metric(knn1_1.trainX, trainx[:, i:i+1]) for i in range(trainx.shape[1])])\n",
        "actual = knn1_1.find_top_neighbor_labels(dists)\n",
        "if np.isclose(actual, expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 4\")\n",
        "#test case 5 \n",
        "expected = np.array([[0, 1, 2], [0, 1, 1], [1, 2, 0], [0, 1, 1], [1, 0, 2]])\n",
        "dists = np.vstack([knn3.metric(knn3.trainX, testx[:, i:i+1]) for i in range(testx.shape[1])])\n",
        "if np.isclose(knn3.find_top_neighbor_labels(dists), expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 5\")\n",
        "#test case 6\n",
        "print(\"testing knn.predict\")\n",
        "expected = np.array([0, 0, 1, 0, 1])\n",
        "if np.isclose(knn1.predict(testx), expected).all(): \n",
        "    passed += 1 \n",
        "else: \n",
        "    print(\"failed test 6\")\n",
        "\n",
        "#test case 7 \n",
        "expected = np.array([0, 1, 2, 1, 0])\n",
        "if np.isclose(knn1_1.predict(testx), expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 7\")\n",
        "\n",
        "#test case 8\n",
        "expected = np.array([0, 1, 0, 1, 0])\n",
        "if np.isclose(knn3.predict(testx), expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 8\")\n",
        "\n",
        "#test case 9\n",
        "expected = np.array([0, 1, 0, 0, 1]) \n",
        "if np.isclose(knn5.predict(testx), expected).all(): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 9\")\n",
        "\n",
        "print(\"testing knn.score\")\n",
        "#test case 10 \n",
        "expected = 0.8\n",
        "if math.isclose(knn3.score(testx, testy_1), expected): \n",
        "    passed += 1 \n",
        "else: \n",
        "    print(\"failed test 10\")\n",
        "\n",
        "#test case 11\n",
        "expected = 0.4\n",
        "if math.isclose(knn5.score(testx, testy_1), expected): \n",
        "    passed += 1\n",
        "else: \n",
        "    print(\"failed test 11\")\n",
        "print(f\"passed {passed}/11 test cases\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing knn.calc_distances\n",
            "testing knn.find_top_neighbor_labels\n",
            "testing knn.predict\n",
            "testing knn.score\n",
            "passed 11/11 test cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeyAXfypwyl-"
      },
      "source": [
        "# 5) NN: Nearest Neighbors or Neural Networks?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6woRs81w41C"
      },
      "source": [
        "Now that we have implemented our K-Nearest Neighbors algorithm, we can test how well it does on Scikit-learn's MNIST dataset.\n",
        "\n",
        "First, we need to process data to fit our class standards. Scikit-learn's input data take the form $(n,d)$ and labels take the form $(n,)$. Implement the function below so that the data can be fed into our KNNs (refer back to the KNN specifications above if you don't remember the needed shapes).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wViQNrUtxqie"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mnist = datasets.load_digits()\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(np.array(mnist.data), mnist.target, test_size=0.25, random_state=42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRT5WzGxw5ZW"
      },
      "source": [
        "def process_data_for_knn(train_data, test_data, train_labels, test_labels):\n",
        "    trainX = train_data.T\n",
        "    testX = test_data.T\n",
        "    trainY = train_labels[np.newaxis,]\n",
        "    testY = test_labels[np.newaxis,]\n",
        "    return trainX, testX, trainY, testY"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIjOhtA1xlK1"
      },
      "source": [
        "Now we'll evaluate the KNN accuracy on various values of K using Euclidean distance and compare its performance to a simple neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKNfVGOL4Hr0"
      },
      "source": [
        "A) What is the accuracy of KNN using Euclidean distance on the MNIST dataset for K=1, 3, 5?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "988wevIGyM6F",
        "outputId": "f4c95e27-697a-4364-e6dc-66e3188a3235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainX, testX, trainY, testY = process_data_for_knn(train_data, test_data, train_labels, test_labels)\n",
        "EucNN1 = KNN(1, euclidean, trainX, trainY)\n",
        "EucNN3 = KNN(3, euclidean, trainX, trainY)\n",
        "EucNN5 = KNN(5, euclidean, trainX, trainY)\n",
        "print(EucNN1.score(testX, testY))\n",
        "print(EucNN3.score(testX, testY))\n",
        "print(EucNN5.score(testX, testY))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9822222222222222\n",
            "0.9866666666666667\n",
            "0.9933333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwffXlD64Soo"
      },
      "source": [
        "Use the code below to test how well a simple Neural network with two hidden layers, of sizes 50 and 5, performs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1899oYJ7TJyX"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "norm_train_data, norm_test_data = train_data/16, test_data/16\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,5), max_iter=10000, alpha=1e-4, solver='sgd', verbose=None, random_state=1,learning_rate_init=.1)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
        "    mlp.fit(norm_train_data, train_labels) \n",
        "    print(mlp.score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUT46-pi4ujI"
      },
      "source": [
        "B) What is the accuracy of the provided Neural Network on the MNIST dataset?\n",
        "\n",
        "\n",
        "C) What performs marginally better on this simple dataset: nearest neighbors or neural networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kmcggkO478N"
      },
      "source": [
        "# 6) Nearest Neighbors for Few-Shot Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWSsSnjA5php"
      },
      "source": [
        "One of the major benefits of KNNs that we hinted at during lab 7 is that they can do surprisingly well in few-shot\n",
        "learning settings, where only a few labeled examples are available. More specifically, we hypothesized that by doing KNNs on embeddings of the examples provided by an auto-encoder, we could get a great performance with ony a few single labeled examples. Let's put it to the test!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSM4tW2s68Ah"
      },
      "source": [
        "We first start by training an auto-encoder. Because we want to test the KNN, not the auto-encoder itself, we will use the entire dataset to train the auto-encoder and produce the embeddings. As always for neural nets, we standardize the data first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SeA_wg_6RJN"
      },
      "source": [
        "mnist = datasets.load_digits()\n",
        "all_data, _, all_labels, _ = train_test_split(np.array(mnist.data), mnist.target, test_size=0.01, random_state=42)\n",
        "all_data = all_data/16  #max int is 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCHBT_0g7Ygr"
      },
      "source": [
        "To simplify the task a little bit, we will only keep 2 classes: zeros and twos. Use the `keep_subset_classes` function below, which takes in a list of integers corresponding to the classes we want to keep, and returns that subset of the data and labels with only those classes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewEeGCYyOMsp"
      },
      "source": [
        "def keep_subset_classes(data, labels, labels_to_include):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        data is (n,d)\n",
        "        labels is (n,) \n",
        "        labels_to_include is a list of unique labels corresponding to the classes to be kept\n",
        "    Returns:\n",
        "        multiclass_data (m,d) where m is the number of examples belonging to the classes in labels_to_include\n",
        "        multiclass_labels (m,) where m is the number of examples belonging to the classes in labels_to_include\n",
        "    Note the shapes are different this time, as they use scikit-learn's convention. \n",
        "    \"\"\"\n",
        "    multiclass_data = data[np.isin(labels, labels_to_include)]\n",
        "    multiclass_labels = labels[np.isin(labels, labels_to_include)]\n",
        "    return multiclass_data, multiclass_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbU37i2u7SLJ"
      },
      "source": [
        "labels_to_include = #TODO: your code here\n",
        "multiclass_data, multiclass_labels = keep_subset_classes(all_data, all_labels, labels_to_include)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksfCFmME5wvL"
      },
      "source": [
        "Next, we define the architecture of an auto-encoder similar to the one we saw in lab 7 and train it on the multiclass data. Use the `MLPRegressor`'s `fit` function to train the auto-encoder. Be careful with what the input data and the output \"labels\" are in this case (remember that an auto-encoder tries to reconstruct the inputs)! Training will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6pXZHwd48vx"
      },
      "source": [
        "n_input = 8*8\n",
        "n_encoder = 128\n",
        "n_latent = 2\n",
        "n_decoder = 128\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "reg = MLPRegressor(hidden_layer_sizes=(n_encoder, n_latent, n_decoder), activation = 'tanh', \n",
        "                   solver = 'adam', learning_rate_init = 0.0001, max_iter = 10000, \n",
        "                   tol = 0.0000001, verbose = False, random_state=2)\n",
        "\n",
        "# TODO: Fit the MLPRegressor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yLUxzOk-KPF"
      },
      "source": [
        "Now that we have trained our auto-encoder, we can retrieve the embeddings (i.e. latent space representation) and the reconstructions. While we don't need the reconstructions per se, they are useful to sanity check that our training was succesful. If you need a refresher on how auto-encoders work, have a look at lab 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaQ_ljVS8NcF"
      },
      "source": [
        "def encoder(data):\n",
        "    data = np.asmatrix(data)\n",
        "    encoder1 = np.tanh(data@reg.coefs_[0] + reg.intercepts_[0])\n",
        "    latent = np.tanh(encoder1@reg.coefs_[1] + reg.intercepts_[1])\n",
        "    return np.asarray(latent)\n",
        "\n",
        "def decoder(new_data):\n",
        "    new_data = np.asmatrix(new_data)\n",
        "    decoder1 = np.tanh(new_data@reg.coefs_[2] + reg.intercepts_[2])\n",
        "    reconst = decoder1@reg.coefs_[3] + reg.intercepts_[3]\n",
        "    return np.asarray(reconst)\n",
        "\n",
        "multiclass_latent = encoder(multiclass_data)\n",
        "multiclass_reconstruct = decoder(multiclass_latent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYw8kJH5P1XP"
      },
      "source": [
        "We can visualize the reconstructions and originals below to ensure the auto-encoder has learned relevant embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDxlb94S-gSA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(pixels):\n",
        "    pixels = pixels.reshape((8,8))\n",
        "    plt.imshow(pixels, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "print('RECONSTRUCTION')\n",
        "visualize(multiclass_reconstruct[30, :]) #our reconstruction\n",
        "print('ORIGINAL')\n",
        "visualize(multiclass_data[30, :]) #the original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vq3d1M1CtXA"
      },
      "source": [
        "The code below is already implemented for you. The two dictionaries `lab2im` and `lab2lat` map labels to the images and embeddings, respectively. The function `generate_train_with_size_and_test`will be useful to generate training sets with only a few labeled examples for every class as well as test sets from these dictionaries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MNNMnuO-vPz"
      },
      "source": [
        "lab2im = dict([(i, []) for i in labels_to_include])\n",
        "for image, label in zip(multiclass_data, multiclass_labels):\n",
        "    lab2im[label].append(image)\n",
        "\n",
        "lab2lat = dict([(i, []) for i in labels_to_include])\n",
        "for image, label in zip(multiclass_latent, multiclass_labels):\n",
        "    lab2lat[label].append(image)\n",
        "\n",
        "def generate_train_with_size_and_test(lab2vec, n_train_per_class, n_test_per_class, iter_number):\n",
        "    train_labels, train_images = [], []\n",
        "    test_labels, test_images = [], []\n",
        "    for k in lab2vec:\n",
        "      train_images.extend(lab2vec[k][iter_number:iter_number+n_train_per_class])\n",
        "      train_labels.extend([[k]]*n_train_per_class)\n",
        "      test_images.extend(lab2vec[k][-n_test_per_class:])\n",
        "      test_labels.extend([[k]]*n_test_per_class)\n",
        "    return np.array(train_images).T, np.array(train_labels).T, np.array(test_images).T, np.array(test_labels).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUR2_sTPLRZs"
      },
      "source": [
        "We've implemented the function `compute_average_score` which does a cross-validation-like evaluation for a given number of labeled training examples per class. \n",
        "\n",
        "Specifically, it computes the average score across `crossval_iterations`. In every iteration, it calls `generate_train_with_size_and_test` with the parameters, including the current iteration number to get a fresh train and test set. Then instantiate and score a new KNN instance, with K=3 unless n_train_per_class is equal to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkmGYOS-_4kS"
      },
      "source": [
        "def compute_average_score(lab2vec, n_train_per_class, n_test_per_class, crossval_iterations):\n",
        "    ''' \n",
        "    Parameters: \n",
        "      lab2vec: either lab2im or lab2lat\n",
        "      n_train_per_class: the number of labeled training samples per class\n",
        "      n_test_per_class: the number of test samples per class\n",
        "      crossval_iterations: the number of iterations to evaluate a KNN\n",
        "    Returns:\n",
        "      score: a float between 0 and 1\n",
        "    '''\n",
        "    total = 0\n",
        "    for i in range(crossval_iterations): \n",
        "      Xtrain, ytrain, Xtest, ytest = generate_train_with_size_and_test(lab2vec, n_train_per_class, n_test_per_class, i)\n",
        "      k = 3 if n_train_per_class != 1 else 1 \n",
        "      model = KNN(k, euclidean, Xtrain, ytrain)\n",
        "      total += model.score(Xtest, ytest)\n",
        "    return total / crossval_iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EOrO7p4Q3SC"
      },
      "source": [
        "Now use `compute_average_score` to test KNN for various numbers of labeled training samples per class. The code below then generates a common plot in machine learning, namely the learning curve: a plot of accuracy versus training set size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm5k2L8iKDRU"
      },
      "source": [
        "#don't modify this\n",
        "test_samples_per_class = 140\n",
        "crossval_iterations = 100\n",
        "\n",
        "num_labeled_samples = [1, 3, 5, 9, 15, 25]\n",
        "embedding_scores = [compute_average_score(lab2lat, i, test_samples_per_class, crossval_iterations) for i in num_labeled_samples]\n",
        "raw_scores = [compute_average_score(lab2im, i, test_samples_per_class, crossval_iterations) for i in num_labeled_samples]\n",
        "\n",
        "print(\"Embedding scores:\", embedding_scores)\n",
        "print(\"Raw Scores:\", raw_scores)\n",
        "\n",
        "plt.plot(num_labeled_samples, embedding_scores, label='embeddings')\n",
        "plt.plot(num_labeled_samples, raw_scores, label='raw')\n",
        "plt.legend()\n",
        "plt.xlabel('number of training samples per class')\n",
        "plt.ylabel('accuracy')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrK-aLz1Qhns"
      },
      "source": [
        "A) What is the mean accuracy using cross-validation of KNN with Euclidean distance on the embeddings when only a single labeled example is available per class? Does this match your expectation?\n",
        "\n",
        "B) What is the mean accuracy using cross-validation of KNN with Euclidean distance on the raw pixels when only a single labeled example is available per class? Does this match your expectation?\n",
        "\n",
        "C) What is the mean accuracy using cross-validation of KNN with Euclidean distance on the embeddings for the following numbers of labeled examples per class: 3, 5, 9, 15, 25?\n",
        "\n",
        "D) What is the mean accuracy using cross-validation of KNN with Euclidean distance on the raw pixels for the following numbers of labeled examples per class: 3, 5, 9, 15, 25?\n",
        "\n",
        "E) What performs best on this dataset when only a few labeled examples are available: doing KNN on the embeddings or on the raw pixels?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr_5Eyet4BTX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}