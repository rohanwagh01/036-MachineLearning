{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 Homework 1 Colab Notebook Fall 2021",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanwagh01/036-MachineLearning/blob/main/MIT_6_036_Homework_1_Colab_Notebook_Fall_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3rlhVsaHpo"
      },
      "source": [
        "#MIT 6.036: HW02 Fall 2021\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkL4viEaJq7u"
      },
      "source": [
        "## Setup\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcyC3YnGJybh"
      },
      "source": [
        "import numpy as np\n",
        "!rm -rf code_for_hw02*\n",
        "!wget --quiet https://go.odl.mit.edu/subject/6.036/_static/catsoop/homework/hw02/code_for_hw02.py --no-check-certificate\n",
        "from code_for_hw02 import *"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LFTpdkHKNv"
      },
      "source": [
        "##4) Regularization and Cross Validation\n",
        "\n",
        "We will now try to synthesize what we've learned in order to perform ridge regression on the datacommons obesity <a href=\"https://docs.google.com/spreadsheets/d/1E7AWa69QUoybnAsBrXYR93Y44isph6-1/edit?usp=sharing&ouid=115847065627713602338&rtpof=true&sd=true\">dataset</a>. Unlike in lab02, where we did some simple linear regressions, here we now employ and explore regularization, with the goal of building a model which generalizes better (than without regularization) to unseen data.\n",
        "\n",
        "The metric we will use to measure the quality of our learned predictors is ** Mean Square Error (MSE). ** This is useful metric because it gives a sense of the deviation in the natural units of the predictor. MSE is defined as:\n",
        "\n",
        "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left( y^{(i)} - f(x^{(i)}) \\right)^2 $$\n",
        "\n",
        "where $f$ is our learned predictor: in this case, $f(x) = \\theta \\cdot x + \\theta_0$. This gives a measure of how far away the true values are from the predicted values.\n",
        "\n",
        "We will use ** ridge regression **, which is defined by this form:\n",
        "\n",
        "$$ \\lambda || \\theta ||^2 $$\n",
        "\n",
        "where $\\lambda$ is the regularization parameter. The overall objective function is thus\n",
        "\n",
        "$$ J_\\text{ridge}(\\theta,\\theta_0)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\theta^Tx^{(i)}+\\theta_0-y^{(i)}\\right)^2+\\lambda||\\theta||^2 $$\n",
        "\n",
        "Remarkably, there is an analytical function giving $\\Theta = (\\theta, \\theta_0)$ which minimizes this objective, given $X$, $Y$, and $\\lambda$. But how should we choose $\\lambda$?\n",
        "\n",
        "To choose an optimum $\\lambda$, we can use the following approach. Each particular value of $\\lambda$ gives us a different linear regression model. And we want the best model: one which balances providing good predictions (fitting well to given training data) with generalizing well (avoiding over-fitting training data). And as we see in the lecture notes, we can employ ** cross-validation ** to evaluate and compare different models.\n",
        "\n",
        "###Implementation of cross-validation algorithm\n",
        "\n",
        "Let us begin by implementing this algorithm for cross-validation:\n",
        "\n",
        "<img src=\"https://go.odl.mit.edu/subject/6.036/_static/catsoop/homework/hw02/IMAGE-cross-validation-algorithm-from-notes.png\">\n",
        "\n",
        "We'll split this into a few parts, and have you implement three short functions that build up to an implementation of the above algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvl0pWUi1jHF"
      },
      "source": [
        "####4A) `lin_reg`\n",
        "\n",
        "We will first implement a generic linear regression function, `lin_reg`, that has the following input arguments:\n",
        "* `x`: the list of data points ($d\\times n$)\n",
        "* `th`: the coefficients of the regression ($d\\times1$)\n",
        "* `th0`: the offset ($1\\times1$)\n",
        "\n",
        "Our function `lin_reg` returns a $1\\times n$ matrix:\n",
        "* `y`: the result of applying the regression on `x`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy6R1Vhou1eG",
        "outputId": "f2511ecd-4f46-4101-8bcf-304806fad44a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def lin_reg(x, th, th0):\n",
        "    return th.T@x+th0\n",
        "\n",
        "test_lin_reg(lin_reg)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqOm1JZf2dxL"
      },
      "source": [
        "####4B) `square_loss`\n",
        "\n",
        "Next, we will implement a function that calculates the squared loss of linear regression, `square_loss`, that has the following input arguments:\n",
        "* `x`: the list of data points ($d\\times n$)\n",
        "* `y`: the true values of the responders ($1\\times n$)\n",
        "* `th`: the coefficients of the regression ($d\\times1$)\n",
        "* `th0`: the offset ($1\\times1$)\n",
        "\n",
        "Our function `square_loss` returns a $1\\times n$ matrix, the squared loss of this linear regression on each data point. A working implementation of `lin_reg` will be available to you on catsoop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OpIdnFC2x6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504b55bd-217e-4e68-89fd-4d7a43d2d38b"
      },
      "source": [
        "def square_loss(x, y, th, th0):\n",
        "    return np.square(lin_reg(x, th, th0) - y)\n",
        "\n",
        "test_square_loss(square_loss)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKVX6GW325nG"
      },
      "source": [
        "####4C) `mean_square_loss`\n",
        "\n",
        "Now, we will implement a function that calculates the mean squared loss of linear regression, `mean_square_loss`, that has the following input arguments:\n",
        "* `x`: the list of data points ($d\\times n$)\n",
        "* `y`: the true values of the responders ($1\\times n$)\n",
        "* `th`: the coefficients of the regression ($d\\times1$)\n",
        "* `th0`: the offset ($1\\times1$)\n",
        "\n",
        "Our function `mean_square_loss` returns a $1\\times1$ matrix, the mean squared loss of this linear regression on the list of data points. Working implementations of both `lin_reg` and `square_loss` will be available to you on catsoop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIhw1naD22Kq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa747c41-5be8-4771-9454-5daf76a38c2a"
      },
      "source": [
        "def mean_square_loss(x, y, th, th0):\n",
        "    return (1/y.shape[1])*np.sum(square_loss(x, y, th, th0))\n",
        "\n",
        "test_mean_square_loss(mean_square_loss)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOUEBnXwE_16"
      },
      "source": [
        "####4D) `cross_validate`\n",
        "\n",
        "Last, we will implement a the above cross-validation algorithm in `cross_validate`, that has the following input arguments:\n",
        "* `X`: the list of data points ($d\\times n$)\n",
        "* `Y`: the true values of the responders ($1\\times n$)\n",
        "* `n_splits`: the number of chunks to divide the dataset into\n",
        "* `lam`: the regularization parameter\n",
        "\n",
        "Our function `cross_validate` returns a scalar, the cross validation error of applying linear regression on the list of data points. Working implementations of `lin_reg`, `square_loss`, and `mean_square_loss` will be available to you in catsoop, along with the following functions:\n",
        "\n",
        "```python\n",
        "def make_splits(X, Y, n_splits):\n",
        "    '''Splits the dataset into n chunks, creating 10 sets of cross validation data.\n",
        "    Returns a list of n tuples (X_train, Y_train, X_test, Y_test)\n",
        "    \n",
        "    X : d x n numpy array (d = # features, n = # data points)\n",
        "    Y : 1 x n numpy array\n",
        "    n_splits: int'''\n",
        "\n",
        "def ridge_analytic(X_train, Y_train, lam=lam):\n",
        "    '''Applies analytic ridge regression on the given training data.\n",
        "    Returns th, th0\n",
        "\n",
        "    X : d x n numpy array (d = # features, n = # data points)\n",
        "    Y : 1 x n numpy array\n",
        "    lam : (float) regularization strength parameter\n",
        "    th : d x 1 numpy array\n",
        "    th0: 1 x 1 numpy array'''\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB5JD-NM8QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c87cc2b-c98d-4d4d-dcc7-682bc61410eb"
      },
      "source": [
        "def cross_validate(X, Y, n_splits, lam):\n",
        "    test_errors = []\n",
        "    for (X_train, Y_train, X_test, Y_test) in make_splits(X, Y, n_splits):\n",
        "        th, th0 = ridge_analytic(X_train, Y_train, lam=lam)\n",
        "        test_errors.append(mean_square_loss(X_test, Y_test, th, th0))\n",
        "    return np.array(test_errors).mean()\n",
        "\n",
        "test_cross_validate(cross_validate)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9SaOwgEe0vw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}