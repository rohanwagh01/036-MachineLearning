{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.036 Fall 2021 HW03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanwagh01/036-MachineLearning/blob/main/6_036_Fall_2021_HW03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmv3jlgr4_Ji"
      },
      "source": [
        "# MIT 6.036 Spring 2021: Homework 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY6MBM3vsUJV"
      },
      "source": [
        "**Setup**\n",
        "\n",
        "**Make a copy of this colab in your own Drive folder before executing. You can do this by going the File > Save a copy in Drive.**\n",
        "\n",
        "Download the code distribution for this homework that contains test cases and helper functions. Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N622h8-D5i-M",
        "outputId": "9c85096a-23e2-4e66-a968-c17058c53931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf code_and_data_for_hw03*\n",
        "!rm -rf mnist\n",
        "!wget --no-check-certificate --quiet https://go.odl.mit.edu/subject/6.036/_static/catsoop/homework/hw03/code_and_data_for_hw03.zip\n",
        "!unzip code_and_data_for_hw03.zip\n",
        "!mv code_and_data_for_hw03/* .\n",
        "\n",
        "from code_for_hw03 import *\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  code_and_data_for_hw03.zip\n",
            "   creating: code_and_data_for_hw03/\n",
            "  inflating: code_and_data_for_hw03/code_for_hw03.py  \n",
            "Importing code_for_hw03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUS51a8m5rEI"
      },
      "source": [
        "## 3) Implementing gradient descent\n",
        "In this section we will implement generic versions of gradient descent and apply these to the logistic regression objective.\n",
        "\n",
        "<b>Note: </b> If you need a refresher on gradient descent,\n",
        "you may want to reference\n",
        "<a href=\"https://canvas.mit.edu/courses/11118/files/1660165?module_item_id=404498\">this week's notes</a>.\n",
        "\n",
        "### 3.1) Implementing Gradient Descent\n",
        "We want to find the $x$ that minimizes the value of the *objective\n",
        "function* $f(x)$, for an arbitrary scalar function $f$.  The function\n",
        "$f$ will be implemented as a Python function of one argument, that\n",
        "will be a numpy column vector.  For efficiency, we will work with\n",
        "Python functions that return not just the value of $f$ at $f(x)$ but\n",
        "also return the gradient vector at $x$, that is, $\\nabla_x f(x)$.\n",
        "\n",
        "We will now implement a generic gradient descent function, `gd`, that\n",
        "has the following input arguments:\n",
        "\n",
        "* `f`: a function whose input is an `x`, a column vector, and\n",
        "  returns a scalar.\n",
        "* `df`: a function whose input is an `x`, a column vector, and\n",
        "  returns a column vector representing the gradient of `f` at `x`.\n",
        "* `x0`: an initial value of $x$, `x0`, which is a column vector.\n",
        "* `step_size_fn`: a function that is given the iteration index (an\n",
        "  integer) and returns a step size.\n",
        "* `num_steps`: the number of iterations to perform\n",
        "\n",
        "Our function `gd` returns a tuple:\n",
        "\n",
        "* x: the value at the final step\n",
        "* fx: the value of f(x) at the final step\n",
        "\n",
        "**Hint:** This is a short function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s03NFuxG6kvt"
      },
      "source": [
        "The main function to implement is `gd`, defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNsLE3bg6jt9"
      },
      "source": [
        "def gd(f, df, x0, step_size_fn, num_steps):\n",
        "    for i in range(num_steps):\n",
        "        x0 = x0 - step_size_fn(i)*df(x0)\n",
        "    return (x0,f(x0))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXu60n-H5_Hz"
      },
      "source": [
        "To evaluate results, we also use a simple `package_ans` function,\n",
        "which checks the final `x` and `fx` values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN_XbacQ6Rue"
      },
      "source": [
        "The test cases are provided below, but you should feel free (and are encouraged!) to write more of your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJcClaqN4nE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d66fe84-a5d3-42a6-d683-ccd236719e42"
      },
      "source": [
        "test_gd(gd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1:\n",
            "Passed!\n",
            "Test 2:\n",
            "Passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbuSt5hY645k"
      },
      "source": [
        "### 3.2) Numerical Gradient\n",
        "Getting the analytic gradient correct for complicated functions is\n",
        "tricky.  A very handy method of verifying the analytic gradient or\n",
        "even substituting for it is to estimate the gradient at a point by\n",
        "means of *finite differences*.\n",
        "\n",
        "Assume that we are given a function $f(x)$ that takes a column vector\n",
        "as its argument and returns a scalar value.  In gradient descent, we\n",
        "will want to estimate the gradient of $f$ at a particular $x_0.$\n",
        "\n",
        "The $i^{th}$ component of $\\nabla_x f(x_0)$ can be estimated as\n",
        "$$\\frac{f(x_0+\\delta^{i}) - f(x_0-\\delta^{i})}{2\\delta}$$\n",
        "where $\\delta^{i}$ is a column vector whose $i^{th}$ coordinate is\n",
        "$\\delta$, a small constant such as 0.001, and whose other components\n",
        "are zero.\n",
        "Note that adding or subtracting $\\delta^{i}$ is the same as\n",
        "incrementing or decrementing the $i^{th}$ component of $x_0$ by\n",
        "$\\delta$, leaving the other components of $x_0$ unchanged.  Using\n",
        "these results, we can estimate the $i^{th}$ component of the gradient.\n",
        "\n",
        "\n",
        "**For example**, take $x^(0) = (1,2,3)^T$. The gradient $\\nabla_x f(x)$ is a vector of the derivatives of $f(x)$ with respect to each component of $x$, or $\\nabla_x f(x) = (\\frac{df(x)}{dx_1},\\frac{df(x)}{dx_2},\\frac{df(x)}{dx_3})^T$.\n",
        "\n",
        "We can approximate the first component of $\\nabla_x f(x)$ as\n",
        "$$\\frac{f((1,2,3)^T+(0.01,0,0)^T) - f((1,2,3)^T-(0.01,0,0)^T)}{2\\cdot 0.01}.$$\n",
        "\n",
        "(We add the transpose so that these are column vectors.)\n",
        "**This process should be done for each dimension independently,\n",
        "and together the results of each computation are compiled to give the\n",
        "estimated gradient, which is $d$ dimensional.**\n",
        "\n",
        "Implement this as a function `make_num_grad_fn` that takes as arguments the\n",
        "objective function `f` and a value of `delta`, and returns a new\n",
        "**function** that takes an `x` (a column vector of parameters) and\n",
        "returns a gradient column vector.\n",
        "\n",
        "**Note:** Watch  out for aliasing. If you do temp_x = x where x is a vector (numpy array), then temp_x is just another name for the same vector as x and changing an entry in one will change an entry in the other. You should either use x.copy() or remember to change entries back after modification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPVwGZ-l6XvW"
      },
      "source": [
        "def make_num_grad_fn(f, delta=0.001):\n",
        "    def df(x):\n",
        "        output = []\n",
        "        for i in range(np.shape(x)[0]):\n",
        "            grad_array = np.array([[0]]*i+[[delta]]+[[0]]*(x.shape[0]-1-i))\n",
        "            output.append((f(x+grad_array)-f(x-grad_array))/(2*delta))\n",
        "        return np.array([output]).T\n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kElTR0bL7cbG"
      },
      "source": [
        "The test cases are shown below; these use the functions defined in the previous exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiWOdSl_6yAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0651383b-b3df-48f0-9dcb-f3d6f598b049"
      },
      "source": [
        "test_num_grad(make_num_grad_fn)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1\n",
            "Passed\n",
            "Test 2\n",
            "Passed\n",
            "Test 3\n",
            "Passed\n",
            "Test 4\n",
            "Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WASaSsYu75sG"
      },
      "source": [
        "A faster (one function evaluation per entry), though sometimes less\n",
        "accurate, estimate is to use:\n",
        "$$\\frac{f(x_0+\\delta^{i}) - f(x_0)}{\\delta}$$\n",
        "for the $i^{th}$ component of $\\nabla_x f(x_0).$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31sdqyG78jD"
      },
      "source": [
        "3.3) Using the Numerical Gradient\n",
        "Recall that our generic gradient descent function takes both a function\n",
        "`f` that returns the value of our function at a given point, and `df`,\n",
        "a function that returns a gradient at a given point.  Write a function\n",
        "`minimize` that takes only a function `f` and uses this function and\n",
        "numerical gradient descent to return the local minimum.  \n",
        "You may use the default of `delta=0.001` for `make_num_grad_fn`.\n",
        "\n",
        "**Hint:** Your definition of `minimize` should call `make_num_grad_fn` exactly\n",
        "once to return a function. Then you may call this function many times in your updates for numerical gradient descent.\n",
        "You should return the same outputs as `gd`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CStwqDem76Bx"
      },
      "source": [
        "def minimize(f, x0, step_size_fn, num_steps):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      See definitions in 4.1\n",
        "    Returns:\n",
        "      same output as gd\n",
        "    \"\"\"\n",
        "    df = make_num_grad_fn(f, delta=0.001)\n",
        "    for i in range(num_steps):\n",
        "        x0 = x0 - step_size_fn(x0)*df(x0)\n",
        "    return (x0,f(x0))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gl0FTby8EQq"
      },
      "source": [
        "The test cases are below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxBLWJFm8DnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe900bb-41cc-45dc-8da3-f285b4d6fb07"
      },
      "source": [
        "test_minimize(minimize)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1\n",
            "Passed\n",
            "Test 2\n",
            "Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVEt8pQjN6oe"
      },
      "source": [
        "### 4) Stochastic gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCs20on2OHbj"
      },
      "source": [
        "We will now write some general python code to implement gradient descent.\n",
        "\n",
        "sgd takes the following as input: (Recall that the stochastic part refers to using a randomly selected point and corresponding label from the given dataset to perform an update. Therefore, your objective function for a given step will need to take this into account.)\n",
        "\n",
        "<pre>\n",
        "  X: a standard data array (d by n)\n",
        "  y: a standard labels row vector (1 by n)\n",
        "  J: a cost function whose input is a data point (a column vector), a label (1 by 1) and a weight vector w (a column vector) (in that order), and which returns a scalar.\n",
        "  dJ: a cost function gradient (corresponding to J) whose input is a data point (a column vector), a label (1 by 1) and a weight vector w (a column vector) (also in that order), and which returns a column vector.\n",
        "  w0: an initial value of weight vector \n",
        "  step_size_fn: a function that is given the (zero-indexed) iteration index (an integer) and returns a step size.\n",
        "  max_iter: the number of iterations to perform\n",
        "</pre>\n",
        "\n",
        "It returns a tuple:\n",
        "\n",
        "<pre>\n",
        "w: the value of the weight vector at the final step\n",
        "fs: the list of values of J found during all the iterations\n",
        "ws: the list of values of intermediate \n",
        "</pre>\n",
        "\n",
        "**Helpful Note**: We recommend that in your implementation, you append the current value of w to **ws** *before* updating it. Similarly, use the current value to compute the corresponding value of the objective and append it to **fs**. Specifically, the first element of fs should be the value of J calculated with w0, and fs should have length max_iter; similarly, the first element of ws should be w0, and ws should have length max_iter. w is the final w updated max_iter iterations.\n",
        "\n",
        "You might find the function np.random.randint(n) useful in your implementation.\n",
        "\n",
        "Hint: This is a short function; our implementation is around 10 lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wOZHgq9Oug-"
      },
      "source": [
        "def sgd(X, y, J, dJ, w0, step_size_fn, max_iter):\n",
        "    fs = []\n",
        "    ws = []\n",
        "    for i in range(max_iter):\n",
        "        n = np.random.randint(X.shape[1])\n",
        "        ws.append(w0)\n",
        "        fs.append(J(X[:,[n]],y[:,n],w0))\n",
        "        w0 = w0-step_size_fn(i)*(dJ(X[:,[n]],y[:,n],w0))\n",
        "    return (w0,fs,ws)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axrM8_wsO1AZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97039ef6-86d5-4294-9fa8-e5ad6a23fe51"
      },
      "source": [
        "# you must have the num_grad function implemented before you can test the sgd function in colab \n",
        "test_sgd(sgd, make_num_grad_fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running test 1\n",
            "running test 2\n",
            "all tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o9cbeNYPbaA"
      },
      "source": [
        "### 5) Tying Everything Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QlcXc72agXq"
      },
      "source": [
        "In the next subsections, we assume that $X$ is a $d \\times n$ matrix, $Y$ is a $1 \\times n$ matrix, and $\\theta$ is a $d \\times 1$ matrix. Rewriting the ridge objective as matrix operations, we find that: \n",
        "\n",
        "$$ J_{\\text{ridge}}(\\theta) = \\frac{1}{n} (\\theta^T X - Y) (\\theta^T X - Y)^T + \\lambda ||\\theta||^2 $$\n",
        "\n",
        "When implementing `objective_func` and `objective_func_grad`, you *do not* need to concatenate a row of ones to `X`. Assume that the `X` input has already been preprocessed as such. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfN-2UC_cKNw"
      },
      "source": [
        "#### 5.1 Gradient Descent and Stochastic Gradient Descent for Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiZDh02Pced7"
      },
      "source": [
        "Write a function for $J_{\\text{ridge}}(\\theta)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o_ggpY_PXgz"
      },
      "source": [
        "def objective_func(X, Y, lam):\n",
        "    \"\"\"\n",
        "    inputs: \n",
        "      X: a (dxn) numpy array. \n",
        "      Y: a (1xn) numpy array \n",
        "      lambda: regularization parameter \n",
        "    outputs: \n",
        "      f : a function that takes in a (dx1) numpy array \"theta\" and returns *as a float* the value of the ridge \n",
        "      regression objective when theta=\"theta\"\n",
        "    \"\"\"\n",
        "    def f(theta): \n",
        "        return (1/X.shape[1]*(theta.T@X-Y)@(theta.T@X-Y).T + lam*theta.T@theta)[0,0]\n",
        "    return f"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxjDqCv5-KOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b838b25-7c8b-4ad1-fc68-b8d737ab744c"
      },
      "source": [
        "test_obj_func(objective_func)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL9FcOX1cV9E"
      },
      "source": [
        "Write a function for $\\nabla J_{\\text{ridge}}(\\theta)$ with respect to $\\theta$. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGjIWe1vPobO"
      },
      "source": [
        "def objective_func_grad(X, Y, lam):\n",
        "    \"\"\"\n",
        "    inputs: \n",
        "      X: a (dxn) numpy array. \n",
        "      Y: a (1xn) numpy array \n",
        "      lambda: regularization parameter \n",
        "    outputs: \n",
        "      df : a function that takes in a (dx1) numpy array \"theta\" and returns the gradient of the ridge regression \n",
        "      objective when theta=\"theta\" \n",
        "    \"\"\"\n",
        "    def df(theta):  \n",
        "        return 2/X.shape[1]*X@(theta.T@X-Y).T + 2*lam*theta\n",
        "    return df "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyP5Evkg-NIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db1a60a-330b-4a0f-acaa-e84d75d0c978"
      },
      "source": [
        "test_d_obj_func(objective_func_grad)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icX8FISmPozV"
      },
      "source": [
        "#### 5.2 Finding the Best Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymWFpDbDqsbR"
      },
      "source": [
        "Let's load the Boston Housing dataset. Our goal is to build a linear regression model (with regularization) to predict the TARGET_VAL (which is the median value of owner-occupied homes) using all other available features in the dataset.\n",
        "\n",
        "For more information about the Boston housing dataset, please visit this [link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). \n",
        "\n",
        "Note that the data pre-processing routine below normalizes each feature. You will learn more about Feature transformations in Week 5.\n",
        "\n",
        "In what follows, we use Cross-Validation to select the best hyperparamters for gradient descent on the ridge regression model. Using the best hyperparameters, we will then make predictions on a reserved test set. You will also compare the results when using the gradient descent based implementation vs the analytic (closed form) solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Mwg5Bnp67n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2998e12-43e8-4191-fda7-8d464d7a1b7d"
      },
      "source": [
        "# Pre-Processing the data and returning the train and test sets.\n",
        "\n",
        "# load the dataset and do some data exploration\n",
        "X_raw, y_raw = load_boston(return_X_y=True)\n",
        "raw_data = np.concatenate((X_raw, y_raw[:, None]), axis=1)\n",
        "xvars = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
        "yvars = [\"TARGET_VAL\"]\n",
        "\n",
        "data = pd.DataFrame(raw_data, columns=xvars+yvars)\n",
        "\n",
        "# Get the train and test splits to be used later.\n",
        "X_train, y_train, X_test, y_test = get_data_splits_with_transforms(data, xvars, yvars)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Z4plUevNrU"
      },
      "source": [
        "**CODE REQUIRED HERE** Before we start using the Boston Housing dataset, let's implement the `ridge_gd` function. Given an input `X_train`, `y_train`, `lam`, `theta` and `step_size_fn`, run gradient descent on `X_train` and `y_train` starting from `theta = (dx1) vector of zeros`. Return the value of $\\theta$ after running 2000 iterations of gradient descent. \n",
        "\n",
        "```\n",
        "inputs: \n",
        "  X_train: a dxn numpy array \n",
        "  y_train: a 1xn numpy array \n",
        "  lam: lambda \n",
        "  step_size_fn: a function that takes in i, the current training iteration, and returns the step size for iteration i \n",
        "\n",
        "outputs: \n",
        "  theta: value of theta after 2000 iterations of gradient descent \n",
        "```\n",
        "**Hint**: `objective_func` and `objective_func_grad` are very useful here! \\\n",
        "**Hint**: You can also use your `gd` function \\\n",
        "**Hint**: Previously, you've minimized f as a function of x. Now, X and y are constant. What variable are you minimizing over now? \\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8ikZvenxl_O"
      },
      "source": [
        "def ridge_gd(X_train, y_train, lam, step_size_fn): \n",
        "  # TODO \n",
        "  # hint: number of iterations = 2000 \n",
        "  # hint: start from theta = (dx1) vector of zeros\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGfxOAIMvuUK"
      },
      "source": [
        "test_ridge_gd(ridge_gd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcyFKWP233ir"
      },
      "source": [
        "**CODE REQUIRED HERE** In `cross_validate_gd`, run 5-fold cross-validation on the X and y dataset. Use gradient descent to train a linear model for the `X`, `y` data. We've provided a for loop that iterates over each split. In this code: \n",
        "\n",
        "```\n",
        "  X_train_split, y_train_split: data to use for training. This is a (d x n) numpy array, where n=the number of datapoints in k-1 folds \n",
        "  X_val_split, y_val_split: data to use for evaluating the model. This is a (d x n) numpy array, where n=the number of datapoints in 1 fold\n",
        "```\n",
        "\n",
        "**Hint**: Use `ridge_gd` here. \\\n",
        "**Hint**: Take a look at the solutions for last week's cross_validate code if you get stuck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX70rLBt33it"
      },
      "source": [
        "def cross_validate_gd(X, y, lam, step_size_fn):\n",
        "  \"\"\"\n",
        "  Returns k-fold cross-validation loss. On each of the k folds, \n",
        "    train a linear regression model using gradient descent. Return \n",
        "    the average loss across the k folds. \n",
        "  \"\"\"\n",
        "  total_loss = 0\n",
        "  kf = KFold(n_splits=5)\n",
        "  for train_index, test_index in kf.split(X, y=y):\n",
        "    X_train_split, y_train_split = X[train_index].T, y[train_index].T\n",
        "    # TODO - train model on X_train_split, y_train_split using gradient descent\n",
        "    # hint - use variables step_size_fn and lam\n",
        "    X_val_split, y_val_split = X[test_index].T, y[test_index].T\n",
        "    # TODO - evaluate model on X_val_split, y_val_split, add loss to total_loss\n",
        "  return total_loss / kf.n_splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMkgs4rfXWMz"
      },
      "source": [
        "Now it's time to run grid search! We are interested in running grid search over $\\lambda \\in \\{{1e-4, 1e-3, \\cdots, 1e-1\\}}$ and $\\eta \\in \\{{1e-6, 1e-5, \\cdots, 1e-2\\}}$. \n",
        "\n",
        "These two cells are ready to run if you've correctly implemented `cross_validate_gd`. Use the outputs of these cells to answer the rest of problem 5.2. \n",
        "\n",
        "We've also already implemented `cross_validate_analytic` for you. This function returns the cross-validation loss for linear regression models trained with the analytic solution for the squared loss equation. \n",
        "\n",
        "**Note: The next two cells print the cross-validation loss, not the testing set loss! Run the last cell in this notebook for the testing set loss.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nttJDNnpc18o"
      },
      "source": [
        "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "lams = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "# This code runs grid search over the training parameters in `learning_rates` and `lams`\n",
        "for rate in learning_rates:\n",
        "  for lam in lams:\n",
        "    learning_rate_fn = lambda i : rate # learning rate = `rate` throughout training\n",
        "    cross_validation_loss = cross_validate_gd(X_train, y_train, lam, learning_rate_fn)\n",
        "    print(f\"Loss on dataset with lambda={lam}, rate={rate} : cross_validation_loss {cross_validation_loss:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvZz6A4NDBF"
      },
      "source": [
        "lams = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "# This code runs grid search over the training parameters in `lams`\n",
        "for lam in lams:\n",
        "  cross_validation_loss = cross_validate_analytic(X_train, y_train, lam).item()\n",
        "  print(f\"Loss on dataset with lambda={lam}: cross_validation_loss {cross_validation_loss:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKWyzPRkQXl"
      },
      "source": [
        "We will now use the best params found above to build a model on the entire training set (X_train, y_train), get the $\\theta$ values and use them to make predictions for the test set (X_test) and evaluate the error using the *actual* values (y_test). We will compare this error for the gradient descent based implementation vs the analytic solution.\n",
        "\n",
        "\n",
        "**CODE REQUIRED HERE**:\n",
        "\n",
        "1. Update **best_lam_gd** and **best_rate_gd** using the best $\\lambda$ and $\\eta$ values you found using **cross_validate_gd**() above.\n",
        "\n",
        "2. Update **best_lam_analytic** using the best $\\lambda$ value found by using **cross_validate_analytic**() above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhJFMnVUjm-Z"
      },
      "source": [
        "#### Using the functions above along with the best performing hyperparams to\n",
        "### determine the test set errors. Please specify the best lambda and learning \n",
        "### rates for the GD and Analytic cases that you found above.\n",
        "\n",
        "# GD\n",
        "best_lam_gd =       ### to be specified\n",
        "best_rate_gd =    ### to be specified\n",
        "\n",
        "# get_gd_predictions() function is defined in the hw03 code you imported at \n",
        "# the very top. Check the code out if you are curious about the implementation.\n",
        "gd_predictions, gd_error = get_gd_predictions_and_error(\n",
        "    objective_func, objective_func_grad, gd, X_train, y_train, X_test, y_test, best_lam_gd, best_rate_gd)\n",
        "\n",
        "# Analytic\n",
        "best_lam_analytic =  ### to be specified\n",
        "\n",
        "# get_analytic_predictions_and_error() function is defined in the hw03 code \n",
        "# you imported at the very top. Check the code out if you are curious about the \n",
        "# implementation.\n",
        "analytic_predictions, analytic_error = get_analytic_predictions_and_error(\n",
        "    X_train, y_train, X_test, y_test, best_lam_analytic)\n",
        "\n",
        "\n",
        "print(f\"Test loss for GD based implementation={gd_error:0.3f}\")\n",
        "print(f\"Test loss for Analytic (closed form) implementation={analytic_error:0.3f}\")\n",
        "\n",
        "\n",
        "#### (Optional) Compare the results by viewing the scatter plots for predictions.\n",
        "plt.scatter(y_test, gd_predictions, color='red', label='GD')\n",
        "plt.scatter(y_test, analytic_predictions, color='blue', label='Analytic')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('Predictions vs Actual Scatter Plot')\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}